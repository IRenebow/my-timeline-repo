{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "1bf58a27-b4cb-4c8b-87f7-879e237fb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### loading the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as patches\n",
    "import ipywidgets as widgets\n",
    "import voila\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import io\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from scipy.interpolate import interp1d\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35249c1e-13e9-4d12-a981-9cd818c92f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a global directory, just use relative folders:\n",
    "# os.chdir(\"/Users/ruofeiguo/CEO Compensation/my-timeline-repo\")\n",
    "data_directory = \"data\"\n",
    "# graph_directory = \"graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced15afe-3e74-4d0b-a455-49277a836304",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         participantid                   fullName  FiscalYear         CIK  \\\n",
      "0             5070045        Duane L. Burnham(2)        1998        1800   \n",
      "1             5029872        Gary P. Coughlan(2)        1998        1800   \n",
      "2             5029873         Joy A. Amundson(1)        1998        1800   \n",
      "3             5070099            K. Frank Austen        1998        1800   \n",
      "4             5029871  Robert L. Parkinson JR(2)        1998        1800   \n",
      "...               ...                        ...         ...         ...   \n",
      "497070          10406        Ronald L. Nelson(1)        2019  1000000036   \n",
      "497071          27486   Pauline D.E. Richards(1)        2019  1000000036   \n",
      "497072          50402      Bruce B. Churchill(1)        2019  1000000036   \n",
      "497073         286521           David B. Wyshner        2019  1000000036   \n",
      "497074         109683       Geoffrey A. Ballotti        2019  1000000036   \n",
      "\n",
      "        currentCEO  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "...            ...  \n",
      "497070         0.0  \n",
      "497071         0.0  \n",
      "497072         0.0  \n",
      "497073         0.0  \n",
      "497074         1.0  \n",
      "\n",
      "[497075 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "df_part = pd.read_csv(os.path.join(data_directory, 'ParticipantFY.csv'), encoding=\"latin1\", low_memory=False)\n",
    "participant_dict = df_part.drop_duplicates(subset=['participantid', 'fullName']).set_index('participantid')['fullName'].to_dict()\n",
    "\n",
    "# df_part = df_part[['participantid', 'fullName', 'FiscalYear','CIK', 'currentCEO']]\n",
    "print(df_part.head)\n",
    "# df_part = df_part.dropna(subset=['participantid'])\n",
    "# df_part = df_part.dropna(subset=['FiscalYear'])\n",
    "# df_part.to_csv(os.path.join(data_directory, 'ParticipantFY.csv'), index=False)\n",
    "\n",
    "# def resolve_ceoname_id_conflicts(df_comp):\n",
    "#     # Step 1: Resolve multiple companyNames for the same CIK\n",
    "#     df_comp_sorted = df_comp.sort_values(by=['participantid', 'FiscalYear', 'fullName'], ascending=[True, False, False])\n",
    "\n",
    "#     # Select the best companyName for each CIK (latest fiscalYear, longest name)\n",
    "#     best_company_mapping = df_comp_sorted.drop_duplicates(subset=['participantid'], keep='first').set_index('participantid')['fullName']\n",
    "\n",
    "    # # Map the best companyName to all rows with that CIK\n",
    "    # df_comp['fullName'] = df_comp['participantid'].map(best_company_mapping)\n",
    "\n",
    "    # # Step 2: Resolve multiple CIKs for the same companyName\n",
    "    # cik_counts = df_comp.groupby('fullName')['participantid'].transform('nunique')  # Unique CIKs per company\n",
    "    # multiple_cik_mask = cik_counts > 1  # Identify companies linked to multiple CIKs\n",
    "\n",
    "    # # Assign ranks to CIKs within the same company\n",
    "    # df_comp['cik_rank'] = df_comp.groupby('fullName')['participantid'].rank(method='dense', ascending=True).astype(\"Int64\")\n",
    "\n",
    "    # # Rename only when needed\n",
    "#     df_comp.loc[multiple_cik_mask, 'fullName'] = df_comp['fullName'] + \"(\" + df_comp['cik_rank'].astype(str) + \")\"\n",
    "\n",
    "#     # Drop the helper column\n",
    "#     df_comp.drop(columns=['cik_rank'], inplace=True)\n",
    "\n",
    "#     # Step 3: Verify One-to-One Mapping\n",
    "#     cik_to_name_counts = df_comp.groupby('participantid')['fullName'].nunique()\n",
    "#     name_to_cik_counts = df_comp.groupby('fullName')['participantid'].nunique()\n",
    "\n",
    "#     assert cik_to_name_counts.max() == 1, \"Some IDs still map to multiple CEOs!\"\n",
    "#     assert name_to_cik_counts.max() == 1, \"Some CEOs still map to multiple IDs!\"\n",
    "    \n",
    "#     # Ensure the shape remains the same\n",
    "#     assert df_comp.shape[0] == original_shape, \"Row count changed! No rows should be dropped.\"\n",
    "\n",
    "#     return df_comp\n",
    "\n",
    "# # Apply the function\n",
    "# original_shape = df_part.shape[0]\n",
    "# df_final = resolve_ceoname_id_conflicts(df_part)\n",
    "# df_final['CIK'] = df_final['CIK'].astype(str).str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "\n",
    "# print(df_final.head)\n",
    "\n",
    "# # Display the final DataFrame\n",
    "# print(df_final.head)\n",
    "# Merge df_part_full['currentCEO'] into df_part based on 'participantid', 'fullName', and 'FiscalYear'\n",
    "# df_part_full['CIK'] = df_part_full['CIK'].astype(str).str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "# df_final['CIK'] = df_part_full['CIK']\n",
    "# df_final['currentCEO'] = df_part_full['currentCEO']\n",
    "# df_final.to_csv(os.path.join(data_directory, 'ParticipantFY.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b543138-796e-4954-b0b5-428bd7a06242",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv(os.path.join(data_directory, 'SumComp.csv'))\n",
    "# base['CIK'] = base['CIK'].astype(str).str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "# # print(base.head)\n",
    "# base = base.merge(df_part[['currentCEO','participantid', 'CIK', 'FiscalYear']], on=['participantid', 'CIK', 'FiscalYear'], how='left')\n",
    "# base = base[base['currentCEO'] == 1.0]\n",
    "# # print(base.head)\n",
    "# base.to_csv(os.path.join(data_directory, 'SumComp.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb88355-b165-4e64-b019-eda84641bd65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grant = pd.read_csv(os.path.join(data_directory, 'GpbaGrant_clean.csv'))\n",
    "# grant_award = pd.read_csv(os.path.join(data_directory, 'GpbaGrant_CEO.csv'))\n",
    "# # print(grant_award.head)\n",
    "# grant = grant.merge(grant_award[['grantID', 'AwardType']], on='grantID', how='left')\n",
    "# print(grant.head)\n",
    "# grant.to_csv(os.path.join(data_directory, 'GpbaGrant_clean.csv'), index=False)\n",
    "# print(grant.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f8b3be2-b434-4475-b659-d1649b29137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.read_csv(os.path.join(data_directory, 'GpbaTime_vest.csv'))\n",
    "# print(time.head)\n",
    "# time_full = pd.read_csv(os.path.join(data_directory, 'GpbaGrant_CEO.csv'))\n",
    "# time_full['CIK'] = time_full['CIK'].astype(str).str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "# print(time_full.head)\n",
    "# time = time.merge(time_full[['CIK','participantid', 'grantID', 'FiscalYear']], on=['CIK','participantid', 'grantID'], how='left')\n",
    "\n",
    "# time = time.merge(df_part[['currentCEO','participantid', 'CIK', 'FiscalYear']], on=['participantid', 'CIK', 'FiscalYear'], how='left')\n",
    "# time = time[time['currentCEO'] == 1.0]\n",
    "# print(time.head)\n",
    "# time.to_csv(os.path.join(data_directory, 'GpbaTime_vest.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a3c2b054-3476-484f-99d8-29cb71847291",
   "metadata": {},
   "outputs": [],
   "source": [
    "award_color_map = {\n",
    "    'Option': 'dimgray', 'reloadOption': 'dimgray', 'phantomOption': 'dimgray',  # Dark grey\n",
    "    'stock': 'lightgray', 'rsu': 'lightgray', 'sarEquity': 'lightgray', \n",
    "    'sarCash': 'lightgray', 'phantomStock': 'lightgray', 'Performance Unit': 'lightgray',  # Light grey\n",
    "}\n",
    "award_label_map = {\n",
    "    'Option': 'Option Award', 'reloadOption': 'Option Award', 'phantomOption': 'Option Award',  # Dark grey\n",
    "    'stock': 'Stock Award', 'rsu': 'Stock Award', 'sarEquity': 'Stock Award', \n",
    "    'sarCash': 'Stock Award', 'phantomStock': 'Stock Award', 'Performance Unit': 'Stock Award',  # Light grey\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a4f6cec4-1573-4a50-a883-0e1232a60d40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_abs = pd.read_csv(os.path.join(data_directory, 'GpbaAbs_vest_clean.csv'), encoding=\"latin1\")\n",
    "\n",
    "# df_abs = df_abs.rename(columns={'periodID': 'periodId'})\n",
    "# print(grant.head)\n",
    "# grant_award = grant[['grantID', 'AwardType']].drop_duplicates()\n",
    "\n",
    "# # Step 2: Map AwardType to awardType using the provided mapping\n",
    "# grant_award['awardType'] = grant_award['AwardType'].map(award_label_map.get).fillna('Cash Award')\n",
    "\n",
    "# # Step 3: Drop duplicates again\n",
    "# grant_award = grant_award.drop_duplicates()\n",
    "# df_abs = df_abs.drop(columns=['AwardType'])\n",
    "\n",
    "# df_abs = df_abs.merge(grant_award[['grantID','awardType']], left_on=['grantId'], right_on=['grantID'], how='left')\n",
    "\n",
    "# df_abs = df_abs.drop(columns=['grantID'])\n",
    "\n",
    "# df_abs.to_csv(os.path.join(data_directory, 'GpbaAbs_vest_clean.csv'), index=False)\n",
    "\n",
    "# df_abs_try = pd.read_csv(os.path.join(data_directory, 'GpbaAbs_CEO_merged.csv'), encoding=\"latin1\")\n",
    "# df_abs_try = df_abs_try.rename(columns={'periodID': 'periodId'})\n",
    "# df_part_cleaned = df_part[['currentCEO', 'participantid', 'CIK', 'FiscalYear']].drop_duplicates()\n",
    "\n",
    "# # Sort so that rows with currentCEO == 1 come first\n",
    "# df_part_cleaned = df_part_cleaned.sort_values(by='currentCEO', ascending=False)\n",
    "\n",
    "# # Drop duplicates keeping the one with currentCEO == 1 if both exist\n",
    "# df_part_cleaned = df_part_cleaned.drop_duplicates(subset=['participantid', 'CIK', 'FiscalYear'], keep='first')\n",
    "\n",
    "# df_abs_try = df_abs_try.merge(df_part_cleaned[['currentCEO','participantid', 'CIK', 'FiscalYear']], left_on=['participantid', 'CIK', 'fiscalYear'], right_on=['participantid', 'CIK', 'FiscalYear'], how='left')\n",
    "\n",
    "# df_abs_try = df_abs_try[df_abs_try['currentCEO'] == 1.0]\n",
    "# # print(df_abs_try.shape)\n",
    "# df_abs_full = pd.read_csv(os.path.join(data_directory, 'GpbaAbs_CEO.csv'), encoding=\"latin1\")\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# # print(df_abs_full.head)\n",
    "# df_abs_try = df_abs_try.merge(df_abs_full[['grantId', 'absId', 'periodID', 'Detail_cleaned']], left_on=['grantId', 'absId', 'periodId'], right_on=['grantId', 'absId', 'periodID'], how='left')\n",
    "# df_abs_try['value'] = df_abs_try[['nonEquityTarget', 'grantDateFV']].bfill(axis=1).iloc[:, 0]\n",
    "# df_abs_try['size'] = df_abs_try['percentVest'] * df_abs_try['value']\n",
    "# def resolve_CIK_conflicts(df_comp):\n",
    "#     # Step 1: Resolve multiple companyNames for the same CIK\n",
    "#     df_comp_sorted = df_comp.sort_values(by=['CIK', 'fiscalYear', 'companyName'], ascending=[True, False, False])\n",
    "\n",
    "#     # Select the best companyName for each CIK (latest fiscalYear, longest name)\n",
    "#     best_company_mapping = df_comp_sorted.drop_duplicates(subset=['CIK'], keep='first').set_index('CIK')['companyName']\n",
    "\n",
    "#     # Map the best companyName to all rows with that CIK\n",
    "#     df_comp['companyName'] = df_comp['CIK'].map(best_company_mapping)\n",
    "\n",
    "#     # Step 2: Resolve multiple CIKs for the same companyName\n",
    "#     cik_counts = df_comp.groupby('companyName')['CIK'].transform('nunique')  # Unique CIKs per company\n",
    "#     multiple_cik_mask = cik_counts > 1  # Identify companies linked to multiple CIKs\n",
    "\n",
    "#     # Assign ranks to CIKs within the same company\n",
    "#     df_comp['cik_rank'] = df_comp.groupby('companyName')['CIK'].rank(method='dense', ascending=True).astype(\"Int64\")\n",
    "\n",
    "#     # Rename only when needed\n",
    "#     df_comp.loc[multiple_cik_mask, 'companyName'] = df_comp['companyName'] + \"(\" + df_comp['cik_rank'].astype(str) + \")\"\n",
    "\n",
    "#     # Drop the helper column\n",
    "#     df_comp.drop(columns=['cik_rank'], inplace=True)\n",
    "\n",
    "#     # Step 3: Verify One-to-One Mapping\n",
    "#     cik_to_name_counts = df_comp.groupby('CIK')['companyName'].nunique()\n",
    "#     name_to_cik_counts = df_comp.groupby('companyName')['CIK'].nunique()\n",
    "\n",
    "#     assert cik_to_name_counts.max() == 1, \"Some CIKs still map to multiple companyNames!\"\n",
    "#     assert name_to_cik_counts.max() == 1, \"Some companyNames still map to multiple CIKs!\"\n",
    "    \n",
    "#     # Ensure the shape remains the same\n",
    "#     assert df_comp.shape[0] == original_shape, \"Row count changed! No rows should be dropped.\"\n",
    "\n",
    "#     return df_comp\n",
    "\n",
    "# original_shape = df_abs_try.shape[0]\n",
    "# df_abs_try = resolve_CIK_conflicts(df_abs_try)\n",
    "# df_abs_try['grantDate'] = df_abs_try.apply(\n",
    "#     lambda row: row['grantDate'].replace('9999', str(row['fiscalYear'])) if '9999' in str(row['grantDate'])\n",
    "#     else (f\"01/01/{row['fiscalYear']}\" if pd.isna(row['grantDate']) or row['grantDate'] == '' else row['grantDate']),\n",
    "#     axis=1\n",
    "# )\n",
    "# # print(df_abs_try.head)\n",
    "# df_abs_try['grantDate'] = pd.to_datetime(df_abs_try['grantDate'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# df_abs_try['vestLow'] = pd.to_numeric(df_abs_try['vestLow'], errors='coerce').fillna(0).astype(int)\n",
    "# df_abs_try['vestHigh'] = pd.to_numeric(df_abs_try['vestHigh'], errors='coerce').fillna(0).astype(int)\n",
    "# df_abs_try['startDate'] = df_abs_try.apply(lambda row: row['grantDate'] + DateOffset(months=row['vestLow']), axis=1)\n",
    "# df_abs_try['endDate'] = df_abs_try.apply(lambda row: row['grantDate'] + DateOffset(months=row['vestHigh']), axis=1)\n",
    "# df_abs_try = df_abs_try[[\n",
    "#     'fiscalYear', 'absId', 'periodId', 'grantId', 'metric', 'size', 'CIK', 'participantid',\n",
    "#     'grantDate', 'startDate', 'endDate', 'companyName', 'percentVest',\n",
    "#     'value', 'currentCEO', 'Detail_cleaned'\n",
    "# ]]\n",
    "# # print(df_abs_try.head)\n",
    "# df_abs_try.to_csv(os.path.join(data_directory, 'GpbaAbs_vest_clean.csv'), index=False)\n",
    "\n",
    "df_rel = pd.read_csv(os.path.join(data_directory, 'GpbaRel_vest.csv'))\n",
    "# df_rel = df_rel.drop(columns=['AwardType'])\n",
    "\n",
    "# df_rel = df_rel.merge(grant_award[['grantID','awardType']], left_on=['grantId'], right_on=['grantID'], how='left')\n",
    "\n",
    "# df_rel = df_rel.drop(columns=['grantID'])\n",
    "\n",
    "# df_rel.to_csv(os.path.join(data_directory, 'GpbaRel_vest.csv'), index=False)\n",
    "\n",
    "# df_rel_try = pd.read_csv(os.path.join(data_directory, 'GpbaRel_CEO_merged.csv'), encoding=\"latin1\")\n",
    "# df_rel_try = df_rel_try.merge(df_part_cleaned[['currentCEO','participantid', 'CIK', 'FiscalYear']], left_on=['participantid', 'CIK', 'fiscalYear'], right_on=['participantid', 'CIK', 'FiscalYear'], how='left')\n",
    "# df_rel_try = df_rel_try[df_rel_try['currentCEO'] == 1.0]\n",
    "# # print(df_rel_try.head)\n",
    "# df_rel_full = pd.read_csv(os.path.join(data_directory, 'GpbaRel_CEO.csv'), encoding=\"latin1\")\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# # print(df_abs_full.head)\n",
    "# df_rel_try = df_rel_try.merge(df_rel_full[['grantId', 'relId', 'periodId', 'Detail_cleaned']], on=['grantId', 'relId', 'periodId'], how='left')\n",
    "# df_rel_try['value'] = df_rel_try[['nonEquityTarget', 'grantDateFV']].bfill(axis=1).iloc[:, 0]\n",
    "# df_rel_try['size'] = df_rel_try['percentVest'] * df_rel_try['value']\n",
    "\n",
    "# original_shape = df_rel_try.shape[0]\n",
    "# df_rel_try = resolve_CIK_conflicts(df_rel_try)\n",
    "# df_rel_try['grantDate'] = df_rel_try.apply(\n",
    "#     lambda row: row['grantDate'].replace('9999', str(row['fiscalYear'])) if '9999' in str(row['grantDate'])\n",
    "#     else (f\"01/01/{row['fiscalYear']}\" if pd.isna(row['grantDate']) or row['grantDate'] == '' else row['grantDate']),\n",
    "#     axis=1\n",
    "# )\n",
    "# df_rel_try['grantDate'] = pd.to_datetime(df_rel_try['grantDate'], format='%m/%d/%Y', errors='coerce')\n",
    "# df_rel_try['vestLow'] = pd.to_numeric(df_rel_try['vestLow'], errors='coerce').fillna(0).astype(int)\n",
    "# df_rel_try['vestHigh'] = pd.to_numeric(df_rel_try['vestHigh'], errors='coerce').fillna(0).astype(int)\n",
    "# df_rel_try['startDate'] = df_rel_try.apply(lambda row: row['grantDate'] + DateOffset(months=row['vestLow']), axis=1)\n",
    "# df_rel_try['endDate'] = df_rel_try.apply(lambda row: row['grantDate'] + DateOffset(months=row['vestHigh']), axis=1)\n",
    "# df_rel_try = df_rel_try[[\n",
    "#     'fiscalYear', 'relId', 'periodId', 'grantId', 'metric', 'size', 'CIK', 'participantid',\n",
    "#     'grantDate', 'startDate', 'endDate', 'companyName', 'percentVest',\n",
    "#     'value', 'currentCEO', 'Detail_cleaned'\n",
    "# ]]\n",
    "# # print(df_rel_try.head)\n",
    "# df_rel_try.to_csv(os.path.join(data_directory, 'GpbaRel_vest.csv'), index=False)\n",
    "# print(df_rel.head)\n",
    "# print(df_abs.head)\n",
    "\n",
    "\n",
    "columns_to_display = [\n",
    "        'CIK', 'participantid', 'companyName', 'metric', 'fiscalYear', \n",
    "        'grantDate', 'startDate', 'endDate', 'size', 'percentVest', 'value', 'grantId', 'periodId'\n",
    "    ]\n",
    "df_abs_filtered = df_abs[columns_to_display]\n",
    "df_rel_filtered = df_rel[columns_to_display]\n",
    "df_abs_filtered = df_abs_filtered.copy()\n",
    "df_rel_filtered = df_rel_filtered.copy()\n",
    "df_abs_filtered.loc[:,'absRel'] = 'abs'\n",
    "df_rel_filtered.loc[:,'absRel'] = 'rel'\n",
    "df_comp = pd.concat([df_abs_filtered, df_rel_filtered], ignore_index=True)\n",
    "\n",
    "df_comp['participantid'] = df_comp['participantid'].astype(int)\n",
    "\n",
    "unique_cik = (\n",
    "    df_comp[['CIK', 'companyName']]\n",
    "    .drop_duplicates()\n",
    "    .values\n",
    "    .tolist()\n",
    ")\n",
    "cik_to_company = {cik: name for cik, name in unique_cik}\n",
    "company_to_cik = {name: cik for cik, name in unique_cik}\n",
    "\n",
    "unique_ceo = (\n",
    "    df_comp[['companyName', 'participantid', 'fiscalYear']]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "unique_ceo = unique_ceo.loc[unique_ceo.groupby(['companyName', 'participantid'])['fiscalYear'].idxmin()]\n",
    "company_to_ceo = (\n",
    "    unique_ceo.sort_values(by=['companyName', 'fiscalYear'])\n",
    "    .groupby('companyName')['participantid']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Function to get participantid list for a given company\n",
    "def get_participants_for_company(company_val):\n",
    "    return company_to_ceo.get(company_val, [])  # Returns empty list if not found\n",
    "\n",
    "all_company_names = (\n",
    "    df_comp['companyName']\n",
    "    .drop_duplicates()\n",
    "    .values\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6f5f2472-7220-4f14-8bb1-1bb14def1e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "######### This chunk can be used to check what uniquely identify an entry\n",
    "if 1: # change to 1 if needed\n",
    "    df = df_rel\n",
    "    # df_unique = df.drop_duplicates()\n",
    "    duplicates = df[df.duplicated(subset=['grantId', 'relId', 'periodId'], keep=False)]\n",
    "    if not duplicates.empty:\n",
    "        # print(\"Duplicate rows found with the following 'absId' and 'periodID' values:\")\n",
    "        print(duplicates.head)\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669da12f-45ff-44af-9da6-1f09a35c2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rel = pd.read_csv(os.path.join(data_directory, 'GpbaRel_vest.csv'))\n",
    "# df_rel['companyName'] = df_rel['CIK'].map(cik_to_company)\n",
    "# df_rel.to_csv(os.path.join(data_directory, 'GpbaRel_vest.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "df01cf53-3856-4c56-9af5-b21eb34e532d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_df_combined(company_name=None, participantid=None, dis=False):\n",
    "    \"\"\"\n",
    "    Reads the CSV files, filters on company_name (substring match, case-insensitive),\n",
    "    CIK, and participantid if they are provided. Returns the combined DataFrame with\n",
    "    columns: [CIK, companyName, metric, fiscalYear, grantDate, startDate, endDate, value, absRel].\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Read CSVs ---\n",
    "    df_abs = pd.read_csv(os.path.join(data_directory, 'GpbaAbs_vest_clean.csv'))\n",
    "    df_rel = pd.read_csv(os.path.join(data_directory, 'GpbaRel_vest.csv'))\n",
    "\n",
    "    #### get CIK from unique list\n",
    "    CIK = company_to_cik[company_name]\n",
    "\n",
    "    # --- Filter the DataFrames ---\n",
    "    df_abs_filtered = df_abs[(df_abs['CIK'] == CIK) & (df_abs['participantid'] == float(participantid))]\n",
    "    df_rel_filtered = df_rel[(df_rel['CIK'] == CIK) & (df_rel['participantid'] == float(participantid))]\n",
    "\n",
    "    if df_abs_filtered.empty and df_rel_filtered.empty:\n",
    "        print(f\"No data found for CIK={CIK}, participant={participantid}.\")\n",
    "        return  # nothing else to plot\n",
    "    \n",
    "    # --- Select columns and label abs/rel ---\n",
    "    columns_to_display = [\n",
    "        'CIK', 'companyName', 'metric', 'fiscalYear', 'awardType',\n",
    "        'grantDate', 'startDate', 'endDate', 'size', 'percentVest', 'value', 'grantId', 'periodId', 'Detail_cleaned'\n",
    "    ]\n",
    "    df_abs_filtered = df_abs_filtered[columns_to_display].copy()\n",
    "    df_rel_filtered = df_rel_filtered[columns_to_display].copy()\n",
    "\n",
    "    df_abs_filtered['absRel'] = 'abs'\n",
    "    df_rel_filtered['absRel'] = 'rel'\n",
    "\n",
    "    df_combined = pd.concat([df_abs_filtered, df_rel_filtered], ignore_index=True)\n",
    "    df_combined = df_combined.sort_values(by=['metric', 'grantDate'], ascending=[True, True])\n",
    "\n",
    "    mask = (\n",
    "        df_combined['percentVest'].isna() &\n",
    "        df_combined['size'].isna() &\n",
    "        (df_combined['metric'] == 'Unknown') &\n",
    "        (df_combined['startDate'] == df_combined['grantDate']) &\n",
    "        (df_combined['grantDate'] == df_combined['endDate'])\n",
    "    )\n",
    "    df_combined = df_combined[~mask]\n",
    "\n",
    "\n",
    "    # Display the combined DataFrame (in notebook)\n",
    "    if dis:\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        display(df_combined)\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "18bfc52c-98a7-4786-87ed-87a28faacce2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_intro_widget(content, is_button=False, grant_id=None):\n",
    "    \"\"\"\n",
    "    Returns a widget (Button or Label) instead of updating VBox immediately.\n",
    "    \"\"\"\n",
    "    if is_button and grant_id is not None:\n",
    "        button = widgets.Button(\n",
    "            description=content,\n",
    "            layout=widgets.Layout(width=\"auto\"),  # Auto width\n",
    "            tooltip=f\"Click to view details for Grant {grant_id}\"\n",
    "        )\n",
    "        def on_button_click(b):\n",
    "            with output_area:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Grant {grant_id} details:\")\n",
    "        button.on_click(on_button_click)\n",
    "        return button  # Return the button instead of appending\n",
    "\n",
    "    else:\n",
    "        return widgets.Label(value=content)  # Return a plain label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "64596096-049a-4e5c-b78d-bddebaa30c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compensation_timeline_by_grant(company_name, participantid, dis=False):\n",
    "    global grant_intro_box\n",
    "    intro_widgets = []\n",
    "    grant_text = widgets.HTML(\n",
    "        f\"<b>Information on metrics and their groupings:</b>\"\n",
    "    )    \n",
    "    intro_widgets.append(grant_text)\n",
    "\n",
    "    df_combined = create_df_combined(company_name=company_name, participantid=participantid, dis=dis)\n",
    "    if df_combined.empty:\n",
    "        print(f\"No data found for company={company_name}, CIK={CIK}, participant={participantid}.\")\n",
    "        return\n",
    "\n",
    "    # Just in case, ensure these are datetimes\n",
    "    df_combined['grantDate'] = pd.to_datetime(df_combined['grantDate'], errors='coerce')\n",
    "    df_combined['startDate'] = pd.to_datetime(df_combined['startDate'], errors='coerce')\n",
    "    df_combined['endDate']   = pd.to_datetime(df_combined['endDate'], errors='coerce')\n",
    "\n",
    "    CIK = company_to_cik[company_name]\n",
    "    \n",
    "    # --- Retrieve base salary data ---\n",
    "    if pd.isna(CIK):  # If company_name is used, map it to CIK\n",
    "        CIK = df_combined['CIK'].iloc[0] if not df_combined.empty else None\n",
    "\n",
    "    base_filtered = base[(base['CIK'] == CIK) & (base['participantid'] == int(participantid))]\n",
    "    time_filtered = time[(time['CIK'] == CIK) & (time['participantid'] == int(participantid))]\n",
    "\n",
    "    # Convert fiscalYear to integer for sorting\n",
    "    base_filtered.loc[:,'FiscalYear'] = pd.to_numeric(base_filtered['FiscalYear'], errors='coerce')\n",
    "    \n",
    "    # --- Compute the minimum value for scaling line thickness ---\n",
    "    max_value = pd.concat([df_combined['size'], base_filtered['salary'], base_filtered['stockAwards'], base_filtered['optionAwards'], time_filtered['grantDateFV']], axis=0).dropna().max()\n",
    "\n",
    "    # Group by grantId\n",
    "    grouped = df_combined.groupby('grantId', dropna=False)\n",
    "    group_grant_dates = {grantId: sub_df['grantDate'].min() for grantId, sub_df in grouped}\n",
    "    sorted_groups = sorted(group_grant_dates.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Prepare the figure\n",
    "    fig_width = 12  # Keep a fixed width\n",
    "    fig_height = max(8, (len(grouped)+1) * 0.5)  # Scale height based on number of items\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "    # For coloring, reuse your color scheme if you want\n",
    "    # palette = sns.color_palette(\"colorblind\", 10)\n",
    "    color_map = {'abs': '#4C72B0', 'rel': '#F4A261'}\n",
    "    \n",
    "    # We'll keep track of the y-position for each group\n",
    "    current_y = 10\n",
    "    y_positions = []\n",
    "    y_labels = []\n",
    "\n",
    "    # Track legend entries\n",
    "    legend_labels = {\n",
    "        'absolute, performance period': False,\n",
    "        'relative, performance period': False,\n",
    "        'Grant Date': False,\n",
    "        'Non-performance-based': False,\n",
    "        'Time-based': False,\n",
    "        'Stock Award': False,\n",
    "        'Cash Award': False,\n",
    "        'Option Award': False\n",
    "    }\n",
    "\n",
    "    # --- Plot Base Salary ---\n",
    "    max_salary_award = 0 if base_filtered['salary'].dropna().empty else max(20 * base_filtered['salary'].dropna().max() / max_value, 0.3)\n",
    "    max_stock_award = 0 if base_filtered['stockAwards'].dropna().empty else max(20 * base_filtered['stockAwards'].dropna().max() / max_value, 0.3)\n",
    "    max_option_award = 0 if base_filtered['optionAwards'].dropna().empty else max(20 * base_filtered['optionAwards'].dropna().max() / max_value, 0.3)\n",
    "    base_salary_y = current_y\n",
    "    stock_y = base_salary_y + max_salary_award/2 + 4 + max_stock_award/2\n",
    "    option_y = stock_y + max_stock_award/2 + 4 + max_option_award/2\n",
    "    ax.axhline(y=base_salary_y + max_salary_award/2 + 2, color='gray', linestyle='dashed', alpha=0.3)\n",
    "    ax.axhline(y=stock_y + max_stock_award/2 + 2, color='gray', linestyle='dashed', alpha=0.3)\n",
    "    for _, row in base_filtered.iterrows():\n",
    "        fiscal_year = int(row['FiscalYear'])\n",
    "        salary_value = row['salary']\n",
    "        salary_height = 0\n",
    "        if pd.notna(salary_value):\n",
    "            height = max(20 * salary_value / max_value, 1)\n",
    "            # Create a rectangle for base salary\n",
    "            base_salary_rect = patches.Rectangle(\n",
    "                (pd.Timestamp(f\"{fiscal_year}-01-01\"), base_salary_y - height / 2),  # (x, y) position\n",
    "                pd.Timedelta(days=365),  # Width covering the full year\n",
    "                height,  # Height scaled to salary value\n",
    "                facecolor='black',\n",
    "                edgecolor='black', \n",
    "                label='Non-performance-based'\n",
    "            )\n",
    "            \n",
    "            ax.add_patch(base_salary_rect)  # Add rectangle to the plot\n",
    "\n",
    "            legend_labels['Non-performance-based'] = True\n",
    "\n",
    "        stock_value = row['stockAwards']\n",
    "        if pd.notna(stock_value):\n",
    "            height = max(20 * stock_value / max_value, 1)\n",
    "            # Create a rectangle for base salary\n",
    "            stock_rect = patches.Rectangle(\n",
    "                (pd.Timestamp(f\"{fiscal_year}-01-01\"), stock_y - height / 2),  # (x, y) position\n",
    "                pd.Timedelta(days=365),  # Width covering the full year\n",
    "                height,  # Height scaled to salary value\n",
    "                facecolor='black',\n",
    "                edgecolor='black', \n",
    "                label='Non-performance-based'\n",
    "            )\n",
    "            \n",
    "            ax.add_patch(stock_rect)  # Add rectangle to the plot\n",
    "            legend_labels['Non-performance-based'] = True\n",
    "\n",
    "        option_value = row['optionAwards']\n",
    "        if pd.notna(option_value):\n",
    "            height = max(20 * option_value / max_value, 1)\n",
    "            # Create a rectangle for base salary\n",
    "            option_rect = patches.Rectangle(\n",
    "                (pd.Timestamp(f\"{fiscal_year}-01-01\"), option_y - height / 2),  # (x, y) position\n",
    "                pd.Timedelta(days=365),  # Width covering the full year\n",
    "                height,  # Height scaled to salary value\n",
    "                facecolor='black',\n",
    "                edgecolor='black', \n",
    "                label='Non-performance-based'\n",
    "            )\n",
    "            \n",
    "            ax.add_patch(option_rect)  # Add rectangle to the plot\n",
    "            legend_labels['Non-performance-based'] = True\n",
    "\n",
    "    current_y = option_y + max_option_award/2 + 2\n",
    "    # --- Plot Time-Based ---\n",
    "    ax.axhline(y=current_y, color='gray', linestyle='dashed', alpha=0.3)\n",
    "    # print(time_filtered)\n",
    "    time_filtered = time_filtered.sort_values(by='grantDate')\n",
    "    time_y = current_y\n",
    "    current_y += 2\n",
    "    all_nan = time_filtered['grantDateFV'].isna().all()\n",
    "    \n",
    "    for _, row in time_filtered.iterrows():\n",
    "        grantDateFV = row['grantDateFV']\n",
    "        sDate = pd.to_datetime(row['startDate'])\n",
    "        eDate = pd.to_datetime(row['endDate'])\n",
    "        gDate = pd.to_datetime(row['grantDate'])\n",
    "        oops = False\n",
    "        if sDate == eDate:\n",
    "            eDate += pd.Timedelta(days=10)\n",
    "            oops = True\n",
    "        \n",
    "        if pd.notna(grantDateFV):\n",
    "            height = max(20 * grantDateFV / max_value, 1)\n",
    "            # Create a rectangle for base salary\n",
    "            rect = patches.Rectangle(\n",
    "                (sDate, current_y),  # (x, y) position\n",
    "                eDate - sDate,  # width\n",
    "                height,  # height in data units\n",
    "                color='#E9C46A',\n",
    "                label='Time-based'\n",
    "            )\n",
    "            \n",
    "            ax.add_patch(rect)  # Add rectangle to the plot\n",
    "            if oops == True:\n",
    "                ax.scatter(gDate, current_y+height/2, color='black', marker='o', s=6, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=0)\n",
    "            else:\n",
    "                ax.scatter(gDate, current_y+height/2, color='black', marker='o', s=6, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=5)\n",
    "            legend_labels['Time-based'] = True\n",
    "            legend_labels['Grant Date'] = True\n",
    "\n",
    "            current_y += height+2\n",
    "        else:\n",
    "            ax.hlines(\n",
    "                y=current_y,\n",
    "                xmin=sDate,\n",
    "                xmax=eDate,\n",
    "                color=\"#E9C46A\",\n",
    "                linestyle=\"dashed\",\n",
    "                linewidth=1.5,  # Adjust thickness of dashed line\n",
    "                label='Time-based' if all_nan else \"\"\n",
    "            )\n",
    "            if oops == True:\n",
    "                ax.scatter(gDate, current_y, color='black', marker='o', s=6, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=0)\n",
    "            else:\n",
    "                ax.scatter(gDate, current_y, color='black', marker='o', s=6, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=5)\n",
    "            legend_labels['Time-based'] = True\n",
    "            legend_labels['Grant Date'] = True\n",
    "            current_y += 2\n",
    "\n",
    "    current_y += 2\n",
    "    time_y = (current_y + time_y)/2\n",
    "        \n",
    "\n",
    "    # --- Plot Performance-Based ---\n",
    "    sorted_grant_ids = sorted(grouped.groups.keys(), key=lambda x: group_grant_dates[x])\n",
    "\n",
    "    for grant_id in sorted_grant_ids:\n",
    "        low = current_y\n",
    "        performance_grouping = grant.loc[grant['grantID'] == grant_id, 'performanceGrouping']\n",
    "    \n",
    "        # If grant_id is found, get the first matching value\n",
    "        performance_grouping = performance_grouping.iloc[0] if not performance_grouping.empty else None\n",
    "        \n",
    "        ax.axhline(y=current_y, color='gray', linestyle='dashed', alpha=0.3)\n",
    "        # old_y = current_y\n",
    "        group_df = grouped.get_group(grant_id)  # Retrieve the actual DataFrame\n",
    "        label_for_group = (f\"grant {grant_id}\" if pd.notna(grant_id) else \"unknown grant\") \n",
    "        group_df = group_df.copy()\n",
    "        group_df['metric_absrel'] = group_df['metric']+group_df['absRel']\n",
    "\n",
    "        novalue = False\n",
    "        if group_df['value'].isna().all():\n",
    "            novalue = True\n",
    "        else:\n",
    "            first_value = group_df['value'].iloc[0] if not group_df['value'].isna().all() else min_value\n",
    "            total_height = max(20 * (first_value / max_value) if max_value else 20,1)\n",
    "            # 1) Check if percentVest has NaNs\n",
    "            if group_df['percentVest'].isna().any():\n",
    "                use_count_based = True\n",
    "                total_count = len(group_df)\n",
    "                group_df['computedVest'] = total_height / total_count  # Equal height for all rows\n",
    "            else:\n",
    "                use_count_based = False\n",
    "                total_percentVest = group_df['percentVest'].sum()\n",
    "                if total_percentVest == 0 or pd.isna(total_percentVest):\n",
    "                    total_percentVest = 1  # Avoid division by zero\n",
    "                group_df['computedVest'] = np.maximum(group_df['percentVest'] / total_percentVest * total_height, 0.8)\n",
    "        \n",
    "        # Identify distinct combos within this group\n",
    "        # Step 1: Create a unique schedule for each metric\n",
    "        # Convert schedule into a tuple set per metric\n",
    "        schedule_dict = {}\n",
    "        for metric, group in group_df.groupby('metric_absrel'):\n",
    "            unique_schedule = group[['grantDate', 'startDate', 'endDate']].drop_duplicates()\n",
    "            schedule = tuple(unique_schedule.apply(tuple, axis=1))\n",
    "            schedule_dict[metric] = schedule\n",
    "        \n",
    "        # Step 2: Group metrics by identical schedules\n",
    "        schedule_to_metrics = {}\n",
    "        for metric, schedule in schedule_dict.items():\n",
    "            if schedule in schedule_to_metrics:\n",
    "                schedule_to_metrics[schedule].append(metric)\n",
    "            else:\n",
    "                schedule_to_metrics[schedule] = [metric]\n",
    "        \n",
    "        # Step 3: Assign group labels to metrics\n",
    "        metric_to_group = {}\n",
    "        for group_idx, (schedule, metrics) in enumerate(schedule_to_metrics.items(), start=1):\n",
    "            for metric in metrics:\n",
    "                metric_to_group[metric] = f\"Group_{group_idx}\"\n",
    "        \n",
    "        # Step 4: Add group information back to the original DataFrame\n",
    "        group_df = group_df.copy()\n",
    "        group_df['metric_group'] = group_df['metric_absrel'].map(metric_to_group)\n",
    "        # print(group_df)\n",
    "\n",
    "        group_df = group_df.dropna(subset=['metric_group'])\n",
    "        further_groups = group_df.groupby(['metric_group'], dropna=False)\n",
    "        \n",
    "        # If only 1 distinct timeline, we label the y-axis with all metrics from the entire group\n",
    "        # else, we label each timeline line with its metrics\n",
    "        if len(further_groups) == 1:\n",
    "            \n",
    "            # print(grant_id,1)\n",
    "            # There's just one combo => we can directly get that\n",
    "            metric_group, metric_df = list(further_groups)[0]\n",
    "            combo_groups = metric_df.groupby(['grantDate', 'startDate', 'endDate'], dropna=False)\n",
    "            \n",
    "            metric_df['rank'] = metric_df.sort_values(['grantDate', 'startDate', 'endDate']) \\\n",
    "                .groupby('metric') \\\n",
    "                .cumcount() + 1\n",
    "            sorted_metrics = list(set(metric_df['metric'].tolist()[::-1]))\n",
    "            metric_label = \", \".join(sorted_metrics) if sorted_metrics else \"No Metric\"\n",
    "\n",
    "            grant_text = widgets.HTML(\n",
    "                f\"<b>Grant ID:</b> {grant_id} <br>\"\n",
    "                f\"<b>Grouping:</b> {performance_grouping} <br>\"\n",
    "                f\"<b>Metrics:</b>\"\n",
    "            )\n",
    "            intro_widgets.append(grant_text)\n",
    "\n",
    "            if not novalue:\n",
    "                metric_df = metric_df.sort_values(by=['absRel', 'computedVest', 'metric', 'rank'], ascending=[True, False, False, False])\n",
    "\n",
    "            metric_buttons = []\n",
    "            for _, row in metric_df.iterrows():\n",
    "                description = f\"{row['metric']} ({row['rank']}): from {row['startDate'].year} to {row['endDate'].year}\"\n",
    "    \n",
    "                if pd.isna(row['Detail_cleaned']):\n",
    "                    # Create plain text instead of a button\n",
    "                    label = widgets.HTML(value=f\"<span style='color:gray'>{description};</span>\")\n",
    "                    metric_buttons.append(label)\n",
    "                else:\n",
    "                    # Create interactive button\n",
    "                    btn = widgets.Button(\n",
    "                        description=description,\n",
    "                        layout=widgets.Layout(width=\"auto\")\n",
    "                    )\n",
    "                    btn.detail_cleaned = row['Detail_cleaned']  # Store hidden detail\n",
    "                    btn.award = row['awardType']  # Store hidden detail\n",
    "                    btn.grant = row['grantId']  # Store hidden detail\n",
    "                    btn.metric = row['metric']+ '(' + str(row['rank']) + ')'\n",
    "                    btn.size = row['size']\n",
    "                    btn.value = pd.notna(row['size'])\n",
    "                    btn.on_click(on_metric_button_click)\n",
    "                    metric_buttons.append(btn)\n",
    "\n",
    "            # Step 3: Arrange text + buttons horizontally\n",
    "            button_box = widgets.HBox(metric_buttons)  # Display buttons in one row\n",
    "            intro_widgets.append(button_box)  # Add button box to the list\n",
    "\n",
    "\n",
    "            current_y += 6\n",
    "            base_y = current_y\n",
    "            \n",
    "            for (gDate, sDate, eDate), sub_df in combo_groups:\n",
    "                # Plot one line\n",
    "                if novalue:\n",
    "                    # print(grant_id,'novalue')\n",
    "                    color = color_map.get(row['absRel'], 'gray')\n",
    "                    ax.hlines(\n",
    "                        y=current_y,\n",
    "                        xmin=sDate,\n",
    "                        xmax=eDate,\n",
    "                        color=color,\n",
    "                        linestyle=\"dashed\",\n",
    "                        linewidth=1.5  # Adjust thickness of dashed line\n",
    "                    )\n",
    "                    ax.scatter(gDate, current_y, color='black', marker='o', s=20, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=5)\n",
    "                    legend_labels['Grant Date'] = True\n",
    "                else:\n",
    "                    sub_df = sub_df.sort_values(by=['absRel', 'computedVest', 'metric', 'rank'], ascending=[False, True, True, True])\n",
    "                    # 3) Compute the vertical stacking based on computedVest\n",
    "                    start_y = current_y\n",
    "                    for _, row in sub_df.iterrows():\n",
    "                        height = row['computedVest']\n",
    "                        color = color_map.get(row['absRel'], 'gray')\n",
    "        \n",
    "                        line_label = 'absolute, performance period' if row['absRel'] == 'abs' else 'relative, performance period'\n",
    "\n",
    "                        # print(grant_id,metric,sDate,eDate,start_y)\n",
    "                        # Create a rectangle patch instead of using `ax.plot`\n",
    "                        rect = patches.Rectangle(\n",
    "                            (sDate, start_y),  # (x, y) position\n",
    "                            eDate - sDate,  # width\n",
    "                            height,  # height in data units\n",
    "                            facecolor=color,\n",
    "                            edgecolor=None,  # Optional: add border for better visibility\n",
    "                            linewidth=0.2,  # Keep a thin border\n",
    "                            label=line_label if not legend_labels[line_label] else None\n",
    "                        )\n",
    "                        \n",
    "                        ax.add_patch(rect)  # Add rectangle to the plot\n",
    "        \n",
    "                        # Draw a dashed line between stacked rectangles (except for the last one)\n",
    "                        if _ != sub_df.index[-1]:  \n",
    "                            ax.hlines(\n",
    "                                y=start_y + height,  # Position at top of current rectangle\n",
    "                                xmin=sDate,\n",
    "                                xmax=eDate,\n",
    "                                color=\"white\",\n",
    "                                linestyle=\"dashed\",\n",
    "                                linewidth=0.5  # Adjust thickness of dashed line\n",
    "                            )\n",
    "        \n",
    "                        \n",
    "                        legend_labels[line_label] = True  # Mark legend entry as used\n",
    "                \n",
    "                        # Move up to stack the next segment\n",
    "                        start_y += height  # Ensure consistent stacking in data units\n",
    "                \n",
    "                    # Plot the grant date marker\n",
    "                    ax.scatter(gDate, (current_y + start_y) / 2, color='black', marker='o', s=6, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=5)\n",
    "                    legend_labels['Grant Date'] = True\n",
    "                    current_y = start_y+3\n",
    "    \n",
    "            # We'll label the entire group's y-tick with the list of metrics\n",
    "            y_positions.append((base_y+ current_y)/2)\n",
    "            y_labels.append(f\"{label_for_group}\")\n",
    "\n",
    "            current_y += 6\n",
    "        \n",
    "        else:\n",
    "            # print(grant_id,0)\n",
    "            # Multiple distinct combos => each timeline gets its own line\n",
    "            # We'll label each line with the relevant metrics\n",
    "            lines_in_group = 0\n",
    "            current_y += 6\n",
    "            count = 0\n",
    "            grant_text = widgets.HTML(\n",
    "                f\"<b>Grant ID:</b> {grant_id} <br>\"\n",
    "                f\"<b>Grouping:</b> {performance_grouping} <br>\"\n",
    "            )\n",
    "            intro_widgets.append(grant_text)\n",
    "            \n",
    "            for metric_group, metric_df in further_groups:\n",
    "                base_y = current_y\n",
    "                count += 1\n",
    "                if count >1:\n",
    "                    current_y += 6\n",
    "                    ax.axhline(y=current_y, color='gray', linestyle='dashed', alpha=0.3)\n",
    "                    current_y += 6\n",
    "                base_y = current_y\n",
    "\n",
    "                metric_df['rank'] = metric_df.sort_values(['grantDate', 'startDate', 'endDate']) \\\n",
    "                    .groupby('metric') \\\n",
    "                    .cumcount() + 1\n",
    "                    \n",
    "                combo_groups = metric_df.groupby(['grantDate', 'startDate', 'endDate'], dropna=False)\n",
    "\n",
    "                sorted_metrics = list(set(metric_df['metric'].tolist()[::-1]))\n",
    "                \n",
    "                metric_label = \", \".join(sorted_metrics) if sorted_metrics else \"No Metric\"\n",
    "    \n",
    "                grant_text = widgets.HTML(\n",
    "                    f\"<b>Metrics:</b>\"\n",
    "                )\n",
    "                intro_widgets.append(grant_text)\n",
    "                if not novalue:\n",
    "                    metric_df = metric_df.sort_values(by=['absRel', 'computedVest', 'metric', 'rank'], ascending=[True, False, False, False])\n",
    "    \n",
    "                metric_buttons = []\n",
    "                for _, row in metric_df.iterrows():\n",
    "                    description = f\"{row['metric']} ({row['rank']}): from {row['startDate'].year} to {row['endDate'].year}\"\n",
    "        \n",
    "                    if pd.isna(row['Detail_cleaned']):\n",
    "                        # Create plain text instead of a button\n",
    "                        label = widgets.HTML(value=f\"<span style='color:gray'>{description};</span>\")\n",
    "                        metric_buttons.append(label)\n",
    "                    else:\n",
    "                        # Create interactive button\n",
    "                        btn = widgets.Button(\n",
    "                            description=description,\n",
    "                            layout=widgets.Layout(width=\"auto\")\n",
    "                        )\n",
    "                        btn.detail_cleaned = row['Detail_cleaned']  # Store hidden detail\n",
    "                        btn.award = row['awardType']  # Store hidden detail\n",
    "                        btn.grant = row['grantId']  # Store hidden detail\n",
    "                        btn.size = row['size']\n",
    "                        btn.value = pd.notna(row['size'])\n",
    "                        btn.metric = row['metric']+ '(' + str(row['rank']) + ')'  # Store hidden detail\n",
    "                        btn.on_click(on_metric_button_click)\n",
    "                        metric_buttons.append(btn)\n",
    "        \n",
    "                # Step 3: Arrange text + buttons horizontally\n",
    "                button_box = widgets.HBox(metric_buttons)  # Display buttons in one row\n",
    "                intro_widgets.append(button_box)  # Add button box to the list\n",
    "\n",
    "                for (gDate, sDate, eDate), sub_df in combo_groups:\n",
    "                    # Plot one line\n",
    "                    if novalue:\n",
    "                        color = color_map.get(row['absRel'], 'gray')\n",
    "                        ax.hlines(\n",
    "                            y=current_y,\n",
    "                            xmin=sDate,\n",
    "                            xmax=eDate,\n",
    "                            color=color,\n",
    "                            linestyle=\"dashed\",\n",
    "                            linewidth=1.5  # Adjust thickness of dashed line\n",
    "                        )\n",
    "                        ax.scatter(gDate, current_y, color='black', marker='o', s=20, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=5)\n",
    "                        legend_labels['Grant Date'] = True\n",
    "                    else:\n",
    "                        sub_df = sub_df.sort_values(by=['absRel', 'computedVest'], ascending=[False, True])\n",
    "                        # 3) Compute the vertical stacking based on computedVest\n",
    "                        start_y = current_y\n",
    "                        for _, row in sub_df.iterrows():\n",
    "                            height = row['computedVest']\n",
    "                            color = color_map.get(row['absRel'], 'gray')\n",
    "            \n",
    "                            line_label = 'absolute, performance period' if row['absRel'] == 'abs' else 'relative, performance period'\n",
    "            \n",
    "                            # Create a rectangle patch instead of using `ax.plot'\n",
    "                            rect = patches.Rectangle(\n",
    "                                (sDate, start_y),  # (x, y) position\n",
    "                                eDate - sDate,  # width\n",
    "                                height,  # height in data units\n",
    "                                facecolor=color,\n",
    "                                edgecolor=None,  # Optional: add border for better visibility\n",
    "                                linewidth=0.2,  # Keep a thin border\n",
    "                                label=line_label if not legend_labels[line_label] else None\n",
    "                            )\n",
    "                            \n",
    "                            ax.add_patch(rect)  # Add rectangle to the plot\n",
    "            \n",
    "                            # Draw a dashed line between stacked rectangles (except for the last one)\n",
    "                            if _ != sub_df.index[-1]:\n",
    "                                ax.hlines(\n",
    "                                    y=start_y + height,  # Position at top of current rectangle\n",
    "                                    xmin=sDate,\n",
    "                                    xmax=eDate,\n",
    "                                    color=\"white\",\n",
    "                                    linestyle=\"dashed\",\n",
    "                                    linewidth=0.5  # Adjust thickness of dashed line\n",
    "                                )\n",
    "            \n",
    "                            \n",
    "                            legend_labels[line_label] = True  # Mark legend entry as used\n",
    "                    \n",
    "                            # Move up to stack the next segment\n",
    "                            start_y += height  # Ensure consistent stacking in data units\n",
    "                    \n",
    "                        # Plot the grant date marker\n",
    "                        ax.scatter(gDate, (current_y + start_y) / 2, color='black', marker='o', s=6, label=\"Grant Date\" if not legend_labels['Grant Date'] else \"\", zorder=5)\n",
    "                        legend_labels['Grant Date'] = True\n",
    "                        current_y = start_y+3\n",
    "                        \n",
    "                # We'll label the entire group's y-tick with the list of metrics\n",
    "                y_positions.append((base_y+ current_y)/2)\n",
    "                y_labels.append(f\"{label_for_group}({count})\")\n",
    "    \n",
    "            current_y += 6\n",
    "\n",
    "        award = grant.loc[grant['grantID'] == grant_id, 'AwardType'].iloc[0] if not grant.loc[grant['grantID'] == grant_id, 'AwardType'].empty else None\n",
    "        color = award_color_map.get(award, 'white')\n",
    "        label = award_label_map.get(award, 'Cash Award')\n",
    "        ax.axhspan(low, current_y, color=color, alpha=1, zorder=-1, label=label)\n",
    "        legend_labels[label] = True\n",
    "        \n",
    "\n",
    "    # Decorate axes\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Grant-level grouping\")\n",
    "\n",
    "    # Format x-axis as Year-Month\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # y-ticks\n",
    "    ax.set_yticks(y_positions + [time_y, option_y, stock_y, base_salary_y])\n",
    "    ax.set_yticklabels(y_labels + [\"Time-Based Awards\", \"Option Awards\", \"Stock Awards\", \"Base Salary\"])\n",
    "\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Ensure Proper Legend ---\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    desired_order = ['absolute, performance period', 'relative, performance period', 'Grant Date', 'Non-performance-based', 'Time-based', 'Stock Award','Cash Award','Option Award']\n",
    "    final_labels = [lbl for lbl in desired_order if legend_labels.get(lbl, False)]\n",
    "    # Replace the handle for \"Cash Award\" with a custom one (with a black border)\n",
    "    custom_handles = []\n",
    "    for lbl in final_labels:\n",
    "        if lbl == 'Cash Award':  # Override existing \"Cash Award\" entry\n",
    "            custom_handles.append(patches.Patch(facecolor='white', edgecolor='black', label='Cash Award'))\n",
    "        else:\n",
    "            custom_handles.append(handles[labels.index(lbl)])\n",
    "    # ax.legend(\n",
    "    #     ordered_handles, final_labels, \n",
    "    #     loc='upper center',  # Places legend at the top center\n",
    "    #     bbox_to_anchor=(0.5, 1.15),  # Moves it above the plot\n",
    "    #     ncol=len(final_labels),  # Spread items in a single row\n",
    "    #     frameon=False  # Optional: Remove legend border\n",
    "    # )\n",
    "    # Create the legend above the graph without shrinking it\n",
    "    fig.subplots_adjust(top=0.8)  # Increase top margin without shrinking the plot\n",
    "    \n",
    "    # Add legend as a separate figure element\n",
    "    fig.legend(\n",
    "        custom_handles, final_labels,\n",
    "        loc='upper center',  # Place the legend at the top center\n",
    "        bbox_to_anchor=(0.5, 1.05),  # Position it above the graph\n",
    "        ncol=len(final_labels),  # Arrange items in one row\n",
    "        frameon=False  # Remove legend border\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"Grant-Level Timeline for {df_combined['companyName'].iloc[0]}, CEO {participant_dict[participantid]}\")\n",
    "    plt.show()\n",
    "    grant_intro_box.children = tuple(intro_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d656e3e9-c0f0-403e-bd55-896cbc1aa7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matrix(matrix):\n",
    "    # Convert to list of lists for easier manipulation\n",
    "    matrix = matrix.values.tolist()\n",
    "    first_row=matrix[0]\n",
    "    last_row = matrix[-1]\n",
    "\n",
    "    # 1) Add a row at the top with value\n",
    "    length=len(matrix)\n",
    "    if length==1:\n",
    "        if float(first_row[0])!=0:\n",
    "            add_row=[\n",
    "                float(first_row[0])*9/ 10,\n",
    "                0,\n",
    "                'n'\n",
    "            ]\n",
    "        else:\n",
    "            add_row=[\n",
    "                float(first_row[0])-1,\n",
    "                0,\n",
    "                'n'\n",
    "            ]\n",
    "    else:\n",
    "        add_row=[\n",
    "                float(first_row[0]) - (float(last_row[0]) - float(first_row[0])) / 10,\n",
    "                0,\n",
    "                'n'\n",
    "            ]\n",
    "    matrix.insert(0, add_row)\n",
    "\n",
    "    # 2) Iterate through the matrix, applying the transformation rules\n",
    "    i = 0\n",
    "    while i < len(matrix) - 1:\n",
    "        current_row = matrix[i]\n",
    "        next_row = matrix[i + 1]\n",
    "        \n",
    "        if current_row[2] == 'n':\n",
    "            # Add a row beneath the current row\n",
    "            new_row = [next_row[0], current_row[1], '']\n",
    "            matrix.insert(i + 1, new_row)\n",
    "            i += 1  # Move to the new row added\n",
    "        i += 1  # Move to the next original row\n",
    "    \n",
    "    # 3) Handle the last row\n",
    "\n",
    "    if last_row[2] == 'y':\n",
    "        second_last_row = matrix[-2]\n",
    "        if second_last_row[0]==last_row[0]:\n",
    "            third_last_row = matrix[-3]\n",
    "            new_row = [\n",
    "                float(last_row[0]) + (float(last_row[0]) - float(third_last_row[0])) / 4,\n",
    "                float(last_row[1]) + (float(last_row[1]) - float(third_last_row[1])) / 4,\n",
    "                ''\n",
    "            ]\n",
    "        else:\n",
    "            new_row = [\n",
    "                float(last_row[0]) + (float(last_row[0]) - float(second_last_row[0])) / 4,\n",
    "                float(last_row[1]) + (float(last_row[1]) - float(second_last_row[1])) / 4,\n",
    "                ''\n",
    "            ]\n",
    "    else:\n",
    "        if length==1:\n",
    "            if float(last_row[0])!=0:\n",
    "                new_row = [\n",
    "                    float(last_row[0]) *11 / 10,\n",
    "                    float(last_row[1]),\n",
    "                    ''\n",
    "                ]\n",
    "            else:\n",
    "                new_row = [\n",
    "                    float(last_row[0])+1,\n",
    "                    float(last_row[1]),\n",
    "                    ''\n",
    "                ]\n",
    "        else:\n",
    "            new_row = [\n",
    "                float(last_row[0]) + (float(last_row[0]) - float(first_row[0])) / 10,\n",
    "                float(last_row[1]),\n",
    "                ''\n",
    "            ]\n",
    "\n",
    "    matrix.append(new_row)\n",
    "\n",
    "    # Convert back to a Panda dataframe\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5434e13a-1754-4918-98e5-685cde81bbf2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "######### A function that defines the pay-performance function:\n",
    "# Define the function for interpolation and extrapolation\n",
    "def interpolate_extrapolate(array):\n",
    "    # Extract the input (x) and output (y) columns\n",
    "    x1 = array[:, 0].astype(float)\n",
    "    #print(x1)\n",
    "    if np.all(x1[:-1] >= x1[1:]): # handle decreasing function\n",
    "        x=-x1\n",
    "    else:\n",
    "        x=x1\n",
    "    #print('x: ',x)\n",
    "    y = array[:, 1].astype(float)\n",
    "    #print('y: ',y)\n",
    "    \n",
    "    # Identify the jumps in the data\n",
    "    jump_indices = np.where(np.diff(x) ==0)[0] + 1\n",
    "    #print('jump_indices: ',jump_indices)\n",
    "    \n",
    "    # Split the data into continuous segments\n",
    "    segments = np.split(np.arange(len(x)), jump_indices)\n",
    "    #print('segments: ', segments)\n",
    "    \n",
    "    # Interpolators for each segment\n",
    "    interpolators = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        if len(segment) > 1:  # Only interpolate if there are at least two points\n",
    "            interpolator = interp1d(x[segment], y[segment], kind='linear', fill_value='extrapolate')\n",
    "        else:\n",
    "            # If the segment has only one point, create a flat extrapolation\n",
    "            def flat_extrap(x_new):\n",
    "                return np.full_like(x_new, y[segment[0]], dtype=float)\n",
    "            interpolator = flat_extrap\n",
    "        \n",
    "        interpolators.append(interpolator)\n",
    "    \n",
    "    if np.all(x1[:-1] <= x1[1:]):\n",
    "        #print('increasing')\n",
    "        # Define a wrapper function that applies the correct interpolator based on the input value\n",
    "        def f(x_new):\n",
    "            y_new = np.zeros_like(x_new, dtype=float)\n",
    "            for i, interpolator in enumerate(interpolators):\n",
    "                #print(i)\n",
    "                #print(x[segments[i][0]])\n",
    "                if i == len(segments) - 1:  # Last segment\n",
    "                    mask = (x_new >= x[segments[i][0]])\n",
    "                else:\n",
    "                    mask = (x_new >= x[segments[i][0]]) & (x_new < x[segments[i+1][0]])\n",
    "                    #print(x[segments[i+1][0]])\n",
    "                y_new[mask] = interpolator(x_new[mask])\n",
    "                #print(mask)\n",
    "            return y_new\n",
    "    elif np.all(x1[:-1] >= x1[1:]):\n",
    "        #print('decreasing')\n",
    "        # Define a wrapper function that applies the correct interpolator based on the input value\n",
    "        def f(x_new):\n",
    "            #print(x_new)\n",
    "            x_used=-x_new\n",
    "            #print(x_used)\n",
    "            y_new = np.zeros_like(x_used, dtype=float)\n",
    "            for i, interpolator in enumerate(interpolators):\n",
    "                #print(i)\n",
    "                #print(x[segments[i][0]])\n",
    "                if i == len(segments) - 1:  # Last segment\n",
    "                    mask = (x_used >= x[segments[i][0]])\n",
    "                else:\n",
    "                    mask = (x_used >= x[segments[i][0]]) & (x_used < x[segments[i+1][0]])\n",
    "                    #print(x[segments[i+1][0]])\n",
    "                y_new[mask] = interpolator(x_used[mask])\n",
    "                #print(mask)\n",
    "            return y_new\n",
    "    return f\n",
    "\n",
    "# Define the function for stock award\n",
    "def interpolate_stock(array):\n",
    "    \n",
    "    # Extract the input (x) and output (y) columns\n",
    "    x = array[:, 0].astype(float)\n",
    "    y = array[:, 1].astype(float)\n",
    "    \n",
    "    # Identify the jumps in the data\n",
    "    jump_indices = np.where(np.diff(x) ==0)[0] + 1\n",
    "    #print(jump_indices)\n",
    "    \n",
    "    # Split the data into continuous segments\n",
    "    segments = np.split(np.arange(len(x)), jump_indices)\n",
    "    #print(segments)\n",
    "    \n",
    "    # Interpolators for each segment\n",
    "    interpolators = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        if len(segment) > 1:  # Only interpolate if there are at least two points\n",
    "            interpolator = interp1d(x[segment], y[segment], kind='linear', fill_value='extrapolate')\n",
    "        else:\n",
    "            # If the segment has only one point, create a flat extrapolation\n",
    "            def flat_extrap(x_new):\n",
    "                return np.full_like(x_new, y[segment[0]], dtype=float)\n",
    "            interpolator = flat_extrap\n",
    "            \n",
    "        interpolators.append(interpolator)\n",
    "    \n",
    "    # Define a wrapper function that applies the correct interpolator based on the input value\n",
    "    def f(x_new):\n",
    "        y_new = np.zeros_like(x_new, dtype=float)\n",
    "        for i, interpolator in enumerate(interpolators):\n",
    "            #print(i)\n",
    "            #print(x[segments[i][0]])\n",
    "            if i == len(segments) - 1:  # Last segment\n",
    "                mask = (x_new >= x[segments[i][0]])\n",
    "            else:\n",
    "                mask = (x_new >= x[segments[i][0]]) & (x_new < x[segments[i+1][0]])\n",
    "                #print(x[segments[i+1][0]])\n",
    "            y_new[mask] = interpolator(x_new[mask])\n",
    "            #print(mask)\n",
    "        return y_new*x_new\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Define the function for option\n",
    "def interpolate_option(array):\n",
    "    \n",
    "    # Extract the input (x) and output (y) columns\n",
    "    x = array[:, 0].astype(float)\n",
    "    y = array[:, 1].astype(float)\n",
    "    \n",
    "    # Identify the jumps in the data\n",
    "    jump_indices = np.where(np.diff(x) ==0)[0] + 1\n",
    "    #print(jump_indices)\n",
    "    \n",
    "    # Split the data into continuous segments\n",
    "    segments = np.split(np.arange(len(x)), jump_indices)\n",
    "    #print(segments)\n",
    "    \n",
    "    # Interpolators for each segment\n",
    "    interpolators = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        if len(segment) > 1:  # Only interpolate if there are at least two points\n",
    "            interpolator = interp1d(x[segment], y[segment], kind='linear', fill_value='extrapolate')\n",
    "        else:\n",
    "            # If the segment has only one point, create a flat extrapolation\n",
    "            def flat_extrap(x_new):\n",
    "                return np.full_like(x_new, y[segment[0]], dtype=float)\n",
    "            interpolator = flat_extrap\n",
    "            \n",
    "        interpolators.append(interpolator)\n",
    "    \n",
    "    # Define a wrapper function that applies the correct interpolator based on the input value\n",
    "    def f(x_new,ex):\n",
    "        y_new = np.zeros_like(x_new, dtype=float)\n",
    "        for i, interpolator in enumerate(interpolators):\n",
    "            #print('this is actually going,', i)\n",
    "            #print(x[segments[i][0]])\n",
    "            if i == len(segments) - 1:  # Last segment\n",
    "                mask = (x_new >= x[segments[i][0]]) & (x_new>ex)\n",
    "            else:\n",
    "                mask = (x_new >= x[segments[i][0]]) & (x_new < x[segments[i+1][0]]) & (x_new>ex)\n",
    "                #print(x[segments[i+1][0]])\n",
    "            y_new[mask] = interpolator(x_new[mask])\n",
    "            #print(mask)\n",
    "        return y_new*(x_new-ex)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e2f3bead-fa99-4869-8d41-e5fcf0a4b80a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This function takes in the 'Detail_cleaned' columns of GpbaAbs_CEO and GpbaRel_CEO (a string) and transform it into a matrix sorted from low to high target.\n",
    "def pay_matrix(detail):\n",
    "    rows = detail.strip(';').split(';')\n",
    "\n",
    "    # Split each row into columns by the '|' delimiter and drop the last empty string\n",
    "    matrix = [row.split('|') for row in rows]\n",
    "\n",
    "    # Strip any leading/trailing whitespace from each element\n",
    "    matrix = [[element.strip() for element in row] for row in matrix]\n",
    "\n",
    "    # Transpose the matrix to work with columns\n",
    "    transposed_matrix = list(zip(*matrix))\n",
    "\n",
    "    # Filter out columns that have all empty values\n",
    "    filtered_matrix = [col for col in transposed_matrix if any(col)]\n",
    "\n",
    "    # Transpose back to get the original row-wise structure\n",
    "    matrix = list(map(list, zip(*filtered_matrix)))\n",
    "\n",
    "    # Convert the matrix to a DataFrame\n",
    "    df_cleaned = pd.DataFrame(matrix)\n",
    "\n",
    "    # Drop columns where all values are empty strings\n",
    "    #df_cleaned = df.loc[:, (df != '').any(axis=0)]\n",
    "    # print(df_cleaned)\n",
    "\n",
    "    ## Detect insufficient data\n",
    "    df_avail=df_cleaned.copy()\n",
    "    df_avail.loc[:, 1] = df_cleaned.loc[:, 1].astype(float)\n",
    "    df_avail.loc[:, 0] = df_cleaned.loc[:, 0].astype(float)\n",
    "    \n",
    "    # Sort the copied DataFrame by the converted column in ascending order\n",
    "    df_sorted = df_avail.sort_values(by=[1,0], ascending=True)\n",
    "    if df_sorted.iloc[0,0]>df_sorted.iloc[-1,0]:\n",
    "        df_sorted = df_avail.sort_values(by=[1, 0], ascending=[True, False])\n",
    "        \n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d5b1ba55-7fb3-418d-aadd-bb399d4f3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_string(text, max_len=20):\n",
    "    # Split the string into chunks of max_len characters and join them with '\\n'\n",
    "    return '\\n'.join(textwrap.wrap(text, width=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "39161142-fe96-4f70-b8c9-87967ba0f262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e277666f944da89db0e37c111997bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h1 style='text-align: center;'>Visualizing CEO Compensation</h1>\"), HTML(value='\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an Output widget to capture logs\n",
    "error_output = widgets.Output()\n",
    "output_area = widgets.Output()\n",
    "button_container = widgets.HBox()\n",
    "\n",
    "# Placeholder dictionary to store generated plots for each participant\n",
    "graph_store = {}\n",
    "\n",
    "def filter_company_names(substring):\n",
    "    \"\"\"Return a list of company names containing 'substring' (case-insensitive).\"\"\"\n",
    "    s_lower = substring.lower()\n",
    "    return [n for n in all_company_names if s_lower in n.lower()]\n",
    "\n",
    "def on_company_typed(change):\n",
    "    if change['name'] == 'value':\n",
    "        typed = change['new'].strip()  # Remove extra whitespace\n",
    "        # Filter company suggestions\n",
    "        company_widget.options = [n for n in all_company_names if typed.lower() in n.lower()]\n",
    "\n",
    "        # Enable button only if the exact company name is in all_company_names\n",
    "        run_button.disabled = typed not in all_company_names\n",
    "\n",
    "        # If typed is an exact known name => narrow CIK options\n",
    "        if typed in all_company_names:\n",
    "            matched_ciks = ciks_for_company(typed)\n",
    "            CIK_widget.options = matched_ciks\n",
    "\n",
    "            # If the current typed CIK isn't in matched_ciks, clear the CIK\n",
    "            if CIK_widget.value not in matched_ciks:\n",
    "                CIK_widget.value = \"\"\n",
    "        else:\n",
    "            CIK_widget.options = []\n",
    "            CIK_widget.value = \"\"\n",
    "            \n",
    "def on_run_button_click(b):\n",
    "    \"\"\"\n",
    "    Clears the output area, then runs the plotting function.\n",
    "    \"\"\"\n",
    "    with error_output:\n",
    "        error_output.clear_output()  # Clear previous messages\n",
    "        metric_output.clear_output()\n",
    "        \n",
    "        company_val = company_widget.value\n",
    "\n",
    "        if company_val not in all_company_names:\n",
    "            print(\"Invalid company name\")  # Display error message\n",
    "            return  # Stop execution\n",
    "\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "        \n",
    "        # Now call your timeline function(s)\n",
    "        participant_list = get_participants_for_company(company_val)\n",
    "\n",
    "        # Clear previous buttons\n",
    "        button_container.children = []\n",
    "        grant_intro_box.children = []\n",
    "    \n",
    "        # Create buttons for each CEO\n",
    "        buttons = [\n",
    "            widgets.Button(description=f\"{participant_dict[participant]}\", layout=widgets.Layout(width=\"200px\", height=\"auto\", overflow=\"visible\"))\n",
    "            for participant in participant_list\n",
    "        ]\n",
    "\n",
    "        # Attach event handlers\n",
    "        for btn, participant in zip(buttons, participant_list):\n",
    "            btn.on_click(lambda b, p=participant: plot_selected_graph(company_val, p))\n",
    "    \n",
    "        # Display buttons\n",
    "        button_container.children = buttons\n",
    "\n",
    "def plot_selected_graph(company_val, participant_val):\n",
    "    \"\"\"\n",
    "    Clears the output area and plots the graph for the selected CEO.\n",
    "    \"\"\"\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        plot_compensation_timeline_by_grant(company_val, participant_val, False)\n",
    "\n",
    "def on_metric_button_click(b):\n",
    "    with metric_output:  # Direct output to the output widget\n",
    "        metric_output.clear_output()\n",
    "        detail = b.detail_cleaned\n",
    "        if pd.isna(detail):\n",
    "            print(\"Detailed pay-performance relationship unavailable\")\n",
    "        else:\n",
    "            award = b.award\n",
    "            grant = b.grant\n",
    "            metr = b.metric\n",
    "            df_sorted=pay_matrix(detail)\n",
    "            \n",
    "            if award == 'Cash Award':\n",
    "                size = b.size\n",
    "                value = b.value\n",
    "                if value:\n",
    "                    df_sorted.iloc[:, 1] = df_sorted.iloc[:, 1] * size/100\n",
    "                df_inter=process_matrix(df_sorted)\n",
    "                function= interpolate_extrapolate(df_inter)\n",
    "                x_under=float(df_inter[0,0])\n",
    "                x_up=float(df_inter[-1,0])\n",
    "                xplot=np.linspace(x_under, x_up, num=1000)\n",
    "                yplot = function(xplot)\n",
    "                # Plotting the curve\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                plt.plot(xplot, yplot, linewidth=4) #, marker='o')\n",
    "                x_dot=df_sorted.iloc[:,0].astype(float).to_numpy()\n",
    "                y_dot = function(x_dot)\n",
    "                plt.plot(x_dot, y_dot, 'o', markersize=8)\n",
    "\n",
    "                # Adding labels and title\n",
    "                plt.text(0.5, 1.02, f\"Grant ID: {grant}, Metric: {metr}\", fontsize=15, ha='center', transform=ax.transAxes)\n",
    "                plt.xlabel(f'Performance Measure ({wrap_string(metr, max_len=20)})' if metr == metr else 'Performance Measure (Unknown)')\n",
    "                if value:\n",
    "                    plt.ylabel('Value of Cash Award ($)')\n",
    "                else:\n",
    "                    plt.ylabel('Value of Cash Award over Target Award')\n",
    "                plt.show()\n",
    "\n",
    "            elif award == 'Stock Award':\n",
    "                df_inter=process_matrix(df_sorted)\n",
    "                function= interpolate_extrapolate(df_inter)\n",
    "                x_under=float(df_inter[0,0])\n",
    "                x_up=float(df_inter[-1,0])\n",
    "                xplot=np.linspace(x_under, x_up, num=1000)\n",
    "                yplot = function(xplot)\n",
    "                # Plotting the curve\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                if 1:\n",
    "                    plt.plot(xplot, yplot, linewidth=4) #, marker='o')\n",
    "\n",
    "                if 1:\n",
    "                    x_dot=df_sorted.iloc[:,0].astype(float).to_numpy()\n",
    "                    y_dot = function(x_dot)\n",
    "                    plt.plot(x_dot, y_dot, 'o', markersize=8)\n",
    "                # ax.axis('off')\n",
    "\n",
    "                # Adding labels and title\n",
    "                # Main title (larger font)\n",
    "                plt.text(0.5, 1.02, f\"Grant ID: {grant}, Metric: {metr}\", fontsize=15, ha='center', transform=ax.transAxes)\n",
    "                plt.xlabel(f'Performance Measure ({wrap_string(metr, max_len=20)})' if metr == metr else 'Performance Measure (Unknown)')\n",
    "                plt.ylabel('Value of Stock Award over Target Award')\n",
    "                \n",
    "                try:\n",
    "                    # Your plotting code here\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(\"Plotting failed:\", e)\n",
    "\n",
    "            elif award == 'Option':\n",
    "                df_inter=process_matrix(df_sorted)\n",
    "                function= interpolate_extrapolate(df_inter)\n",
    "                x_under=float(df_inter[0,0])\n",
    "                x_up=float(df_inter[-1,0])\n",
    "                xplot=np.linspace(x_under, x_up, num=1000)\n",
    "                yplot = function(xplot)\n",
    "                # Plotting the curve\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                if 1:\n",
    "                    plt.plot(xplot, yplot, linewidth=4) #, marker='o')\n",
    "\n",
    "                if 1:\n",
    "                    x_dot=df_sorted.iloc[:,0].astype(float).to_numpy()\n",
    "                    y_dot = function(x_dot)\n",
    "                    plt.plot(x_dot, y_dot, 'o', markersize=8)\n",
    "                # ax.axis('off')\n",
    "\n",
    "                # Adding labels and title\n",
    "                # Main title (larger font)\n",
    "                plt.text(0.5, 1.02, f\"Grant ID: {grant}, Metric: {metr}\", fontsize=15, ha='center', transform=ax.transAxes)\n",
    "                plt.xlabel(f'Performance Measure ({wrap_string(metr, max_len=20)})' if metr == metr else 'Performance Measure (Unknown)')\n",
    "                plt.ylabel('Value of Option Award over Target Award')\n",
    "                plt.show\n",
    "                \n",
    "            # print(f\"The pay-performance relationship for metric: {detail}\")\n",
    "\n",
    "# --- Create dynamic Comboboxes for Company & CIK ---\n",
    "company_widget = widgets.Combobox(\n",
    "    placeholder='Type a company name...',\n",
    "    options=all_company_names,\n",
    "    description='Company:'\n",
    ")\n",
    "company_widget.value = \"Apple INC\"  # your existing default\n",
    "company_widget.observe(on_company_typed, names='value')\n",
    "\n",
    "run_button = widgets.Button(description=\"Generate Timeline\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Create a title widget\n",
    "title_widget = widgets.HTML(\n",
    "    value=\"<h1 style='text-align: center;'>Visualizing CEO Compensation</h1>\"\n",
    ")\n",
    "\n",
    "intro_text = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style=\"padding: 10px; border: 1px solid #ccc; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "        <h2 style=\"margin-top: 0;\">How to Use This Dashboard</h2>\n",
    "        <ol style=\"line-height: 1.5;\">\n",
    "            <li><strong>Type a company name</strong> in the search box. Suggestions will appear as you type.</li>\n",
    "            <li>Once a valid company is selected, the <strong>\"Generate Timeline\"</strong> button will activate.</li>\n",
    "            <li><strong>Click \"Generate Timeline\"</strong> to display CEO buttons associated with the company.</li>\n",
    "            <li><strong>Click a CEO button</strong> to view their compensation timeline.</li>\n",
    "            <li>Scroll down to see grant information with performance metrics.</li>\n",
    "            <li><strong>Click a metric button</strong> to view the pay-performance curve (if available).</li>\n",
    "            <li><strong>Click \"Clear\"</strong> to reset and search a new company.</li>\n",
    "        </ol>\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "run_button.on_click(on_run_button_click)\n",
    "\n",
    "# Create the Clear button\n",
    "clear_button = widgets.Button(description=\"Clear\", button_style='warning')\n",
    "\n",
    "# Function to clear input fields\n",
    "def on_clear_button_click(b):\n",
    "    button_container.children = []\n",
    "    grant_intro_box.children = []\n",
    "    with metric_output:\n",
    "        metric_output.clear_output()\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "    graph_store.clear()\n",
    "    company_widget.value = \"\"\n",
    "\n",
    "# Attach event to Clear button\n",
    "clear_button.on_click(on_clear_button_click)\n",
    "grant_intro_box = widgets.VBox([])\n",
    "\n",
    "metric_output = widgets.Output()\n",
    "\n",
    "# Display the widgets & output in the notebook\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        title_widget,  # Add the title at the top\n",
    "        intro_text,\n",
    "        widgets.HBox([company_widget]),\n",
    "        widgets.HBox([run_button, clear_button]),\n",
    "        error_output,\n",
    "        button_container,\n",
    "        output_area,\n",
    "        metric_output,\n",
    "        grant_intro_box\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a360a4b-5a8b-4bb4-be2c-80577c4b73de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f0ad7-f5eb-48e1-a2c1-d969a7371f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
